{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "train = pd.read_csv('./data/input/train.csv', parse_dates=['datetime'], index_col='datetime')\n",
    "test = pd.read_csv('./data/input/test.csv', parse_dates=['datetime'], index_col='datetime')\n",
    "data = pd.concat([train, test], axis=0).sort_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = data.copy()\n",
    "data_0['year'] = data_0.index.year\n",
    "data_0['month'] = data_0.index.month\n",
    "# data['day'] = data.index.day\n",
    "data_0['hour'] = data_0.index.hour\n",
    "data_0['weekday'] = data_0.index.weekday\n",
    "\n",
    "train_0 = data_0.loc[train.index, :]\n",
    "test_0 = data_0.loc[test.index, :].drop(columns=['count', 'casual', 'registered'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仮説検証"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仮説 1\n",
    "count を registered と casual の和として予測する．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t_1_1 : count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf.fit(train_0.drop(columns=['casual', 'registered', 'count']), train_0.loc[:, 'count'])\n",
    "pred = rf.predict(test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test_0.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_1_1.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t_1_2 : registered + casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_0.drop(columns=['casual', 'registered', 'count']), train_0.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_0)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_0.drop(columns=['casual', 'registered', 'count']), train_0.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_0)\n",
    "\n",
    "pred = pred_r + pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test_0.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_1_2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R: 1_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仮説 2 : windspeed\n",
    "風速 0 について"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 2-1 : delete\n",
    "風速 を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete windspeed\n",
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_0.drop(columns=['casual', 'registered', 'count', 'windspeed']), train_0.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_0.drop(columns=['windspeed']))\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_0.drop(columns=['casual', 'registered', 'count', 'windspeed']), train_0.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_0.drop(columns=['windspeed']))\n",
    "\n",
    "pred = pred_r + pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_2_1.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 2-2 : mean\n",
    "風速 0 を平均値に置き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2_2 = data_0.copy()\n",
    "data_2_2.loc[data_2_2.loc[:, 'windspeed'] == 0, 'windspeed'] = data_2_2.loc[data_2_2.loc[:, 'windspeed'] != 0, 'windspeed'].mean()\n",
    "\n",
    "train_2_2 = data_2_2.loc[train.index, :]\n",
    "test_2_2 = data_2_2.loc[test.index, :].drop(columns=['casual', 'registered', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_2_2.drop(columns=['casual', 'registered', 'count']), train_2_2.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_2_2)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_2_2.drop(columns=['casual', 'registered', 'count']), train_2_2.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_2_2)\n",
    "\n",
    "pred = pred_r + pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_2_2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 2-3 : interpolate\n",
    "風速 0 を線形補完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2_3 = data_0.copy()\n",
    "data_2_3.replace({'windspeed': {0: np.nan}}, inplace=True)\n",
    "data_2_3.interpolate(method='time', inplace=True, limit_direction='both')\n",
    "\n",
    "train_2_3 = data_2_3.loc[train.index, :]\n",
    "test_2_3 = data_2_3.loc[test.index, :].drop(columns=['casual', 'registered', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_2_3.drop(columns=['casual', 'registered', 'count']), train_2_3.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_2_3)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_2_3.drop(columns=['casual', 'registered', 'count']), train_2_3.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_2_3)\n",
    "\n",
    "pred = pred_r + pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_2_3.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 2-4 : predict by RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2_4 = data_0.copy()\n",
    "\n",
    "rf_w = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_w.fit(data_2_4.loc[data_2_4.loc[:, 'windspeed'] != 0, :].drop(columns=['casual', 'registered', 'count', 'windspeed']),\n",
    "         data_2_4.loc[data_2_4.loc[:, 'windspeed'] != 0, 'windspeed'])\n",
    "pred_w = rf_w.predict(data_2_4.loc[data_2_4.loc[:, 'windspeed'] == 0, :].drop(columns=['casual', 'registered', 'count', 'windspeed']))\n",
    "\n",
    "data_2_4.loc[data_2_4.loc[:, 'windspeed'] == 0, 'windspeed'] = pred_w\n",
    "\n",
    "train_2_4 = data_2_4.loc[train.index, :]\n",
    "test_2_4 = data_2_4.loc[test.index, :].drop(columns=['casual', 'registered', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_2_4.drop(columns=['casual', 'registered', 'count']), train_2_4.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_2_4)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_2_4.drop(columns=['casual', 'registered', 'count']), train_2_4.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_2_4)\n",
    "\n",
    "pred = pred_r + pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_2_4.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R: 2-1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仮説 3 : weather\n",
    "天候 4 を 3 とする．  \n",
    "仮説 1, 仮説 2-1 の手法を用いる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = data_0.copy()\n",
    "data_3.drop(columns=['windspeed'], inplace=True)\n",
    "data_3.replace({'weather': {4: 3}}, inplace=True)\n",
    "\n",
    "train_3 = data_3.loc[train.index, :]\n",
    "test_3 = data_3.loc[test.index, :].drop(columns=['casual', 'registered', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_3.drop(columns=['casual', 'registered', 'count']), train_3.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_3)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_3.drop(columns=['casual', 'registered', 'count']), train_3.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_3)\n",
    "\n",
    "pred = pred_r + pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_3.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仮説 4 : atemp\n",
    "atemp を使用しない．  \n",
    "仮説 3 の下で行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4 = data_0.copy()\n",
    "data_4.drop(columns=['windspeed', 'atemp'], inplace=True)\n",
    "data_4.replace({'weather': {4: 3}}, inplace=True)\n",
    "\n",
    "train_4 = data_4.loc[train.index, :]\n",
    "test_4 = data_4.loc[test.index, :].drop(columns=['casual', 'registered', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_4.drop(columns=['casual', 'registered', 'count']), train_4.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_4)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_4.drop(columns=['casual', 'registered', 'count']), train_4.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_4)\n",
    "\n",
    "pred = pred_r + pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_4.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仮説 5 : log\n",
    "目的変数を対数変換する．log(x + 1)  \n",
    "仮説 3 の下で行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5 = data_0.copy()\n",
    "data_5.drop(columns=['windspeed'], inplace=True)\n",
    "data_5.replace({'weather': {4: 3}}, inplace=True)\n",
    "\n",
    "train_5 = data_5.loc[train.index, :]\n",
    "test_5 = data_5.loc[test.index, :].drop(columns=['casual', 'registered', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_5.drop(columns=['casual', 'registered', 'count']), np.log1p(train_5.loc[:, 'registered']))\n",
    "pred_r = np.expm1(rf_r.predict(test_5))\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_5.drop(columns=['casual', 'registered', 'count']), np.log1p(train_5.loc[:, 'casual']))\n",
    "pred_c = np.expm1(rf_c.predict(test_5))\n",
    "\n",
    "pred = pred_r + pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_5.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仮説 6 : scaler\n",
    "連続変数を標準化する．  \n",
    "仮説 5 の下で行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_6 = data_0.copy()\n",
    "data_6.drop(columns=['windspeed'], inplace=True)\n",
    "data_6.replace({'weather': {4: 3}}, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_6.loc[:, ['temp', 'atemp', 'humidity']] = scaler.fit_transform(data_6.loc[:, ['temp', 'atemp', 'humidity']])\n",
    "\n",
    "train_6 = data_6.loc[train.index, :]\n",
    "test_6 = data_6.loc[test.index, :].drop(columns=['casual', 'registered', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_6.drop(columns=['casual', 'registered', 'count']), np.log1p(train_6.loc[:, 'registered']))\n",
    "pred_r = np.expm1(rf_r.predict(test_6))\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_6.drop(columns=['casual', 'registered', 'count']), np.log1p(train_6.loc[:, 'casual']))\n",
    "pred_c = np.expm1(rf_c.predict(test_6))\n",
    "\n",
    "pred = pred_r + pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_6.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仮説 7 : datetime\n",
    "datetime について  \n",
    "仮説 6 の下で行う"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 7-1 : month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_7_1 = data_0.copy()\n",
    "data_7_1.drop(columns=['windspeed', 'month'], inplace=True)\n",
    "data_7_1.replace({'weather': {4: 3}}, inplace=True)\n",
    "\n",
    "train_7_1 = data_7_1.loc[train.index, :]\n",
    "test_7_1 = data_7_1.loc[test.index, :].drop(columns=['casual', 'registered', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_7_1.drop(columns=['casual', 'registered', 'count']), np.log1p(train_7_1.loc[:, 'registered']))\n",
    "pred_r = np.expm1(rf_r.predict(test_7_1))\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_7_1.drop(columns=['casual', 'registered', 'count']), np.log1p(train_7_1.loc[:, 'casual']))\n",
    "pred_c = np.expm1(rf_c.predict(test_7_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test.index, 'count': pred_r + pred_c})\n",
    "submission.to_csv('./data/output/t_7_1.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 7-2 : season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_7_2 = data_0.copy()\n",
    "data_7_2.drop(columns=['windspeed', 'season'], inplace=True)\n",
    "data_7_2.replace({'weather': {4: 3}}, inplace=True)\n",
    "\n",
    "train_7_2 = data_7_2.loc[train.index, :]\n",
    "test_7_2 = data_7_2.loc[test.index, :].drop(columns=['casual', 'registered', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_7_2.drop(columns=['casual', 'registered', 'count']), np.log1p(train_7_2.loc[:, 'registered']))\n",
    "pred_r = np.expm1(rf_r.predict(test_7_2))\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_7_2.drop(columns=['casual', 'registered', 'count']), np.log1p(train_7_2.loc[:, 'casual']))\n",
    "pred_c = np.expm1(rf_c.predict(test_7_2))\n",
    "\n",
    "pred = pred_r + pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test.index, 'count': pred})\n",
    "submission.to_csv('./data/output/t_7_2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 7-3 : weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number                       descriptions     rmsle\n",
      "11     40               [atemp] : Do nothing  0.324384\n",
      "12     41                  [atemp] : Deleted  0.325791\n",
      "13     50  [registered, casual] : Do nothing  0.324384\n",
      "14     51         [registered, casual] : Log  0.302610\n",
      "15     60                         Do nothing  0.302610\n",
      "16     61                     StandardScaler  0.302298\n",
      "17     70            [datetime] : Do nothing  0.302298\n",
      "18     71                  [month] : Deleted  0.310272\n",
      "19     72                 [season] : Deleted  0.302249\n",
      "20     73                [weekday] : deleted  0.326193\n"
     ]
    }
   ],
   "source": [
    "# Delete weekday\n",
    "sc = RF_Model_Sum_Logex(train_X_6.drop(columns=['weekday'], axis=1), train_y, valid_X_6.drop(columns=['weekday'], axis=1), valid_y)\n",
    "scores = add_score('73', '[weekday] : deleted', sc, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 7-4 : day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day\n",
    "train_X_6_day = pd.DataFrame(train_X_6.index.day, index=train_X_6.index).rename(columns={'datetime': 'day'})\n",
    "valid_X_6_day = pd.DataFrame(valid_X_6.index.day, index=valid_X_6.index).rename(columns={'datetime': 'day'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number                       descriptions     rmsle\n",
      "12     41                  [atemp] : Deleted  0.325791\n",
      "13     50  [registered, casual] : Do nothing  0.324384\n",
      "14     51         [registered, casual] : Log  0.302610\n",
      "15     60                         Do nothing  0.302610\n",
      "16     61                     StandardScaler  0.302298\n",
      "17     70            [datetime] : Do nothing  0.302298\n",
      "18     71                  [month] : Deleted  0.310272\n",
      "19     72                 [season] : Deleted  0.302249\n",
      "20     73                [weekday] : deleted  0.326193\n",
      "21     74                      [day] : Added  0.300317\n"
     ]
    }
   ],
   "source": [
    "sc = RF_Model_Sum_Logex(\n",
    "    pd.concat([train_X_6, train_X_6_day], axis=1),\n",
    "    train_y,\n",
    "    pd.concat([valid_X_6, valid_X_6_day], axis=1),\n",
    "    valid_y\n",
    ")\n",
    "scores = add_score('74', '[day] : Added', sc, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仮説 8 : datetime, SIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saparate in order\n",
    "train_8 = pd.concat([pd.concat([train_X_6, train_y], axis=1), pd.concat([valid_X_6, valid_y], axis=1)], axis=0).sort_index()\n",
    "\n",
    "train_X_8 = train_8.loc[train_8.index.day <= 14, :].drop(columns=train_y.columns)\n",
    "valid_X_8 = train_8.loc[train_8.index.day > 14, :].drop(columns=train_y.columns)\n",
    "\n",
    "train_y_8 = train_8.loc[train_X_8.index, train_y.columns]\n",
    "valid_y_8 = train_8.loc[valid_X_8.index, train_y.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7620, 11) (8026, 11)\n"
     ]
    }
   ],
   "source": [
    "print(train_X_6.shape, train_X_8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = RF_Model_Sum_Logex(train_X_8, train_y_8, valid_X_8, valid_y_8)\n",
    "scores = add_score('80', 'separate in order, sorted', sc, scores, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number                descriptions     rmsle\n",
      "14     51  [registered, casual] : Log  0.302610\n",
      "15     60                  Do nothing  0.302610\n",
      "16     61              StandardScaler  0.302298\n",
      "17     70     [datetime] : Do nothing  0.302298\n",
      "18     71           [month] : Deleted  0.310272\n",
      "19     72          [season] : Deleted  0.302249\n",
      "20     73         [weekday] : deleted  0.326193\n",
      "21     74               [day] : Added  0.300317\n",
      "22     80   separate in order, sorted  0.320048\n",
      "23     81  separate in order, shuffle  0.320231\n"
     ]
    }
   ],
   "source": [
    "# shuffle\n",
    "shuffle_index = np.random.permutation(train_X_8.index)\n",
    "sc = RF_Model_Sum_Logex(train_X_8.loc[shuffle_index, :], train_y_8.loc[shuffle_index, :], valid_X_8, valid_y_8)\n",
    "scores = add_score('81', 'separate in order, shuffle', sc, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number                     descriptions     rmsle\n",
      "17     70          [datetime] : Do nothing  0.302298\n",
      "18     71                [month] : Deleted  0.310272\n",
      "19     72               [season] : Deleted  0.302249\n",
      "20     73              [weekday] : deleted  0.326193\n",
      "21     74                    [day] : Added  0.300317\n",
      "22     80        separate in order, sorted  0.320048\n",
      "23     81       separate in order, shuffle  0.320231\n",
      "24     82    SIO sorted, [month] : deleted  0.342248\n",
      "25     83   SIO sorted, [season] : deleted  0.319927\n",
      "26     84  SIO sorted, [weekday] : deleted  0.350749\n"
     ]
    }
   ],
   "source": [
    "# SIO sorted, delete month\n",
    "sc = RF_Model_Sum_Logex(train_X_8.drop(columns=['month'], axis=1), train_y_8, valid_X_8.drop(columns=['month'], axis=1), valid_y_8)\n",
    "scores = add_score('82', 'SIO sorted, [month] : deleted', sc, scores, False)\n",
    "\n",
    "#SIO sorted, delete season\n",
    "sc = RF_Model_Sum_Logex(train_X_8.drop(columns=['season'], axis=1), train_y_8, valid_X_8.drop(columns=['season'], axis=1), valid_y_8)\n",
    "scores = add_score('83', 'SIO sorted, [season] : deleted', sc, scores, False)\n",
    "\n",
    "# SIO sorted, delete weekday\n",
    "sc = RF_Model_Sum_Logex(train_X_8.drop(columns=['weekday'], axis=1), train_y_8, valid_X_8.drop(columns=['weekday'], axis=1), valid_y_8)\n",
    "scores = add_score('84', 'SIO sorted, [weekday] : deleted', sc, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number                     descriptions     rmsle\n",
      "19     72               [season] : Deleted  0.302249\n",
      "20     73              [weekday] : deleted  0.326193\n",
      "21     74                    [day] : Added  0.300317\n",
      "22     80        separate in order, sorted  0.320048\n",
      "23     81       separate in order, shuffle  0.320231\n",
      "24     82    SIO sorted, [month] : deleted  0.342248\n",
      "25     83   SIO sorted, [season] : deleted  0.319927\n",
      "26     84  SIO sorted, [weekday] : deleted  0.350749\n",
      "27     85        SIO sorted, [day] : Added  0.319094\n",
      "28     86       SIO shuffle, [day] : Added  0.318503\n"
     ]
    }
   ],
   "source": [
    "# SIO sorted, add day\n",
    "train_X_8_day = pd.DataFrame(train_X_8.index.day, index=train_X_8.index).rename(columns={'datetime': 'day'})\n",
    "valid_X_8_day = pd.DataFrame(valid_X_8.index.day, index=valid_X_8.index).rename(columns={'datetime': 'day'})\n",
    "\n",
    "sc = RF_Model_Sum_Logex(\n",
    "    pd.concat([train_X_8, train_X_8_day], axis=1),\n",
    "    train_y_8,\n",
    "    pd.concat([valid_X_8, valid_X_8_day], axis=1),\n",
    "    valid_y_8\n",
    ")\n",
    "scores = add_score('85', 'SIO sorted, [day] : Added', sc, scores, False)\n",
    "\n",
    "# SIO shuffle, add day\n",
    "sc = RF_Model_Sum_Logex(\n",
    "    pd.concat([train_X_8, train_X_8_day], axis=1).loc[shuffle_index, :],\n",
    "    train_y_8.loc[shuffle_index, :],\n",
    "    pd.concat([valid_X_8, valid_X_8_day], axis=1),\n",
    "    valid_y_8\n",
    ")\n",
    "scores = add_score('86', 'SIO shuffle, [day] : Added', sc, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仮説 9 : humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of zero in humidity:  22\n"
     ]
    }
   ],
   "source": [
    "# check the number of zero in humidity\n",
    "print('The number of zero in humidity: ', train_0.loc[train_0.loc[:, 'humidity'] == 0, 'humidity'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_9 = pd.concat([train_X_8, train_X_8_day], axis=1).sample(frac=1, random_state=0)\n",
    "valid_X_9 = pd.concat([valid_X_8, valid_X_8_day], axis=1)\n",
    "\n",
    "train_y_9 = train_y_8.loc[train_X_9.index, :]\n",
    "valid_y_9 = valid_y_8.loc[valid_X_9.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number                     descriptions     rmsle\n",
      "20     73              [weekday] : deleted  0.326193\n",
      "21     74                    [day] : Added  0.300317\n",
      "22     80        separate in order, sorted  0.320048\n",
      "23     81       separate in order, shuffle  0.320231\n",
      "24     82    SIO sorted, [month] : deleted  0.342248\n",
      "25     83   SIO sorted, [season] : deleted  0.319927\n",
      "26     84  SIO sorted, [weekday] : deleted  0.350749\n",
      "27     85        SIO sorted, [day] : Added  0.319094\n",
      "28     86       SIO shuffle, [day] : Added  0.318503\n",
      "29     90          [humidity] : Do nothing  0.319291\n"
     ]
    }
   ],
   "source": [
    "# Do nothing\n",
    "sc = RF_Model_Sum_Logex(train_X_9, train_y_9, valid_X_9, valid_y_9)\n",
    "scores = add_score('90', '[humidity] : Do nothing', sc, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 9-1 : delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete humidity\n",
    "sc = RF_Model_Sum_Logex(train_X_9.drop(columns=['humidity'], axis=1), train_y_9, valid_X_9.drop(columns=['humidity'], axis=1), valid_y_9)\n",
    "scores = add_score('91', '[humidity] : Deleted', sc, scores, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 9-2: mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_9_2 = train_0.copy()\n",
    "train_9_2.drop(columns=['season', 'windspeed'], axis=1)\n",
    "train_9_2.replace({'weather': 4}, 3).replace({'humidity': 0}, train_9_2.loc[train_9_2.loc[:, 'humidity'] != 0, 'humidity'].mean(), inplace=True)\n",
    "train_9_2.loc[:, 'day'] = train_9_2.index.day\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_9_2.loc[:, ['temp', 'atemp', 'humidity']] = scaler.fit_transform(train_9_2.loc[:, ['temp', 'atemp', 'humidity']])\n",
    "\n",
    "train_X_9_2 = train_9_2.loc[train_X_9.index, train_X_9.columns]\n",
    "valid_X_9_2 = train_9_2.loc[valid_X_9.index, train_X_9.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number                          descriptions     rmsle\n",
      "22     80             separate in order, sorted  0.320048\n",
      "23     81            separate in order, shuffle  0.320231\n",
      "24     82         SIO sorted, [month] : deleted  0.342248\n",
      "25     83        SIO sorted, [season] : deleted  0.319927\n",
      "26     84       SIO sorted, [weekday] : deleted  0.350749\n",
      "27     85             SIO sorted, [day] : Added  0.319094\n",
      "28     86            SIO shuffle, [day] : Added  0.318503\n",
      "29     90               [humidity] : Do nothing  0.319291\n",
      "30     91                  [humidity] : Deleted  0.333830\n",
      "31     92  [humidity] : Replaced zero with mean  0.319134\n"
     ]
    }
   ],
   "source": [
    "# Replace zero with mean\n",
    "sc = RF_Model_Sum_Logex(train_X_9_2, train_y_9, valid_X_9_2, valid_y_9)\n",
    "scores = add_score('92', '[humidity] : Replaced zero with mean', sc, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 9-3 : interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_9_3 = train_0.copy()\n",
    "train_9_3.drop(columns=['season', 'windspeed'], axis=1)\n",
    "train_9_3.replace({'weather': 4}, 3).replace({'humidity': 0}, np.nan, inplace=True)\n",
    "train_9_3.interpolate(method='time', inplace=True, limit_direction='both')\n",
    "train_9_3.loc[:, 'day'] = train_9_3.index.day\n",
    "\n",
    "train_X_9_3 = train_9_3.loc[train_X_9.index, train_X_9.columns]\n",
    "valid_X_9_3 = train_9_3.loc[valid_X_9.index, train_X_9.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number                                 descriptions     rmsle\n",
      "23     81                   separate in order, shuffle  0.320231\n",
      "24     82                SIO sorted, [month] : deleted  0.342248\n",
      "25     83               SIO sorted, [season] : deleted  0.319927\n",
      "26     84              SIO sorted, [weekday] : deleted  0.350749\n",
      "27     85                    SIO sorted, [day] : Added  0.319094\n",
      "28     86                   SIO shuffle, [day] : Added  0.318503\n",
      "29     90                      [humidity] : Do nothing  0.319291\n",
      "30     91                         [humidity] : Deleted  0.333830\n",
      "31     92         [humidity] : Replaced zero with mean  0.319134\n",
      "32     93  [humidity] : Replaced zero with interpolate  0.319005\n"
     ]
    }
   ],
   "source": [
    "sc = RF_Model_Sum_Logex(train_X_9_3, train_y_9, valid_X_9_3, valid_y_9)\n",
    "scores = add_score('93', '[humidity] : Replaced zero with interpolate', sc, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 仮説 9-4 : RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_9_4 = train_9_3.copy()\n",
    "train_9_4.loc[:, 'humidity'] = train_0.loc[train_9_4.index, 'humidity']\n",
    "\n",
    "model = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "model.fit(train_9_4.loc[train_9_4.loc[:, 'humidity'] != 0, :].drop(columns=['registered', 'casual', 'count', 'humidity'], axis=1),\n",
    "            train_9_4.loc[train_9_4.loc[:, 'humidity'] != 0, 'humidity'])\n",
    "pred = model.predict(train_9_4.loc[train_9_4.loc[:, 'humidity'] == 0, :].drop(columns=['registered', 'casual', 'count', 'humidity'], axis=1))\n",
    "\n",
    "train_9_4.loc[train_9_4.loc[:, 'humidity'] == 0, 'humidity'] = pred\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_9_4.loc[:, 'humidity'] = scaler.fit_transform(train_9_4.loc[:, ['humidity']])\n",
    "\n",
    "train_X_9_4 = train_9_4.loc[train_X_9.index, train_X_9.columns]\n",
    "valid_X_9_4 = train_9_4.loc[valid_X_9.index, train_X_9.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number                                 descriptions     rmsle\n",
      "24     82                SIO sorted, [month] : deleted  0.342248\n",
      "25     83               SIO sorted, [season] : deleted  0.319927\n",
      "26     84              SIO sorted, [weekday] : deleted  0.350749\n",
      "27     85                    SIO sorted, [day] : Added  0.319094\n",
      "28     86                   SIO shuffle, [day] : Added  0.318503\n",
      "29     90                      [humidity] : Do nothing  0.319291\n",
      "30     91                         [humidity] : Deleted  0.333830\n",
      "31     92         [humidity] : Replaced zero with mean  0.319134\n",
      "32     93  [humidity] : Replaced zero with interpolate  0.319005\n",
      "33     94           [humidity] : Replaced zero with RF  0.320194\n"
     ]
    }
   ],
   "source": [
    "sc = RF_Model_Sum_Logex(train_X_9_4, train_y_9, valid_X_9_4, valid_y_9)\n",
    "scores = add_score('94', '[humidity] : Replaced zero with RF', sc, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season' 'holiday' 'workingday' 'weather' 'temp' 'atemp' 'humidity'\n",
      " 'windspeed' 'casual' 'registered' 'count' 'year' 'month' 'hour' 'weekday']\n",
      "['season' 'holiday' 'workingday' 'weather' 'temp' 'atemp' 'humidity'\n",
      " 'windspeed' 'year' 'month' 'hour' 'weekday']\n"
     ]
    }
   ],
   "source": [
    "print(train_0.columns.values)\n",
    "print(test_0.columns.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 1\n",
    "- predict count by sum of registered and casual\n",
    "- log convertion and scaler\n",
    "- datetime: year, month, day, weekday\n",
    "- delete: windspeed, season\n",
    "- weather: 4 -> 3\n",
    "- humidity: interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s_1 = pd.concat([train_0, test_0], axis=0)\n",
    "data_s_1.loc[:, 'day'] = data_s_1.index.day\n",
    "data_s_1.drop(columns=['season', 'windspeed'], axis=1, inplace=True)\n",
    "data_s_1.replace({'weather': 4}, 3).replace({'humidity': 0}, np.nan, inplace=True)\n",
    "data_s_1.interpolate(method='time', inplace=True, limit_direction='both')\n",
    "\n",
    "numerical_cols = ['temp', 'atemp', 'humidity']\n",
    "scaler = StandardScaler()\n",
    "data_s_1.loc[:, numerical_cols] = scaler.fit_transform(data_s_1.loc[:, numerical_cols])\n",
    "\n",
    "train_s_1 = data_s_1.loc[train_0.index, :]\n",
    "train_s_1.loc[:, 'registered'] = np.log1p(train_s_1.loc[:, 'registered'])\n",
    "train_s_1.loc[:, 'casual'] = np.log1p(train_s_1.loc[:, 'casual'])\n",
    "\n",
    "test_s_1 = data_s_1.loc[test_0.index, :].drop(columns=['registered', 'casual', 'count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_s_1.drop(columns=['registered', 'casual', 'count'], axis=1), train_s_1.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_s_1)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_s_1.drop(columns=['registered', 'casual', 'count'], axis=1), train_s_1.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_s_1)\n",
    "\n",
    "pred = np.expm1(pred_r) + np.expm1(pred_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test_0.index, 'count': pred})\n",
    "submission.to_csv('./data/output/submission_s_1.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2\n",
    "**1->deleted day**\n",
    "- predict count by sum of registered and casual\n",
    "- log convertion and scaler\n",
    "- datetime: year, month, weekday\n",
    "- delete: windspeed, season\n",
    "- weather: 4 -> 3\n",
    "- humidity: interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s_2 = train_s_1.drop(columns=['day'], axis=1)\n",
    "test_s_2 = test_s_1.drop(columns=['day'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_s_2.drop(columns=['registered', 'casual', 'count'], axis=1), train_s_2.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_s_2)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_s_2.drop(columns=['registered', 'casual', 'count'], axis=1), train_s_2.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_s_2)\n",
    "\n",
    "pred = np.expm1(pred_r) + np.expm1(pred_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test_0.index, 'count': pred})\n",
    "submission.to_csv('./data/output/submission_s_2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 3\n",
    "**2->using windspeed interpolated**\n",
    "- predict count by sum of registered and casual\n",
    "- log convertion and scaler\n",
    "- datetime: year, month, weekday\n",
    "- delete: season\n",
    "- weather: 4 -> 3\n",
    "- humidity, windspeed: interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s_3 = train_s_2.copy()\n",
    "test_s_3 = test_s_2.copy()\n",
    "\n",
    "wind = pd.DataFrame(pd.concat([train_0.loc[:, 'windspeed'], test_0.loc[:, 'windspeed']], axis=0), columns=['windspeed'])\n",
    "wind.replace({'windspeed': 0}, np.nan, inplace=True)\n",
    "wind.interpolate(method='time', inplace=True, limit_direction='both')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "wind.loc[:, 'windspeed'] = scaler.fit_transform(wind.loc[:, ['windspeed']])\n",
    "\n",
    "train_s_3.loc[:, 'windspeed'] = wind.loc[train_s_3.index]\n",
    "test_s_3.loc[:, 'windspeed'] = wind.loc[test_s_3.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_s_3.drop(columns=['registered', 'casual', 'count'], axis=1), train_s_3.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_s_3)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_s_3.drop(columns=['registered', 'casual', 'count'], axis=1), train_s_3.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_s_3)\n",
    "\n",
    "pred = np.expm1(pred_r) + np.expm1(pred_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({'datetime': test_0.index, 'count': pred})\n",
    "submission.to_csv('./data/output/submission_s_3.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 4\n",
    "**3->wind, humidity: no convertion**\n",
    "- predict count by sum of registered and casual\n",
    "- log convertion and scaler\n",
    "- datetime: year, month, weekday\n",
    "- delete: season\n",
    "- weather: 4 -> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s_4 = train_s_3.copy()\n",
    "test_s_4 = test_s_3.copy()\n",
    "\n",
    "train_s_4.loc[:, 'windspeed'] = train_0.loc[:, 'windspeed']\n",
    "train_s_4.loc[:, 'humidity'] = train_0.loc[:, 'humidity']\n",
    "test_s_4.loc[:, 'windspeed'] = test_0.loc[:, 'windspeed']\n",
    "test_s_4.loc[:, 'humidity'] = test_0.loc[:, 'humidity']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_s_4.loc[:, ['windspeed', 'humidity']] = scaler.fit_transform(train_s_4.loc[:, ['windspeed', 'humidity']])\n",
    "test_s_4.loc[:, ['windspeed', 'humidity']] = scaler.transform(test_s_4.loc[:, ['windspeed', 'humidity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_s_4.drop(columns=['registered', 'casual', 'count'], axis=1), train_s_4.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_s_4)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_s_4.drop(columns=['registered', 'casual', 'count'], axis=1), train_s_4.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_s_4)\n",
    "\n",
    "pred = np.expm1(pred_r) + np.expm1(pred_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test_0.index, 'count': pred})\n",
    "submission.to_csv('./data/output/submission_s_4.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 5\n",
    "**3->atemp: delete**\n",
    "- predict count by sum of registered and casual\n",
    "- log convertion and scaler\n",
    "- datetime: year, month, weekday\n",
    "- delete: season, atemp\n",
    "- weather: 4 -> 3\n",
    "- humidity, windspeed: interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s_5 = train_s_3.copy()\n",
    "test_s_5 = test_s_3.copy()\n",
    "\n",
    "train_s_5.drop(columns=['atemp'], axis=1, inplace=True)\n",
    "test_s_5.drop(columns=['atemp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_r.fit(train_s_5.drop(columns=['registered', 'casual', 'count'], axis=1), train_s_5.loc[:, 'registered'])\n",
    "pred_r = rf_r.predict(test_s_5)\n",
    "\n",
    "rf_c = RFR(random_state=0, n_jobs=-1, max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=400)\n",
    "rf_c.fit(train_s_5.drop(columns=['registered', 'casual', 'count'], axis=1), train_s_5.loc[:, 'casual'])\n",
    "pred_c = rf_c.predict(test_s_5)\n",
    "\n",
    "pred = np.expm1(pred_r) + np.expm1(pred_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test_0.index, 'count': pred})\n",
    "submission.to_csv('./data/output/submission_s_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_bike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
